{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from gensim.models import KeyedVectors\n",
    "import wasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2v = wasp.NodeEmbedding(str(wasp.get_data_path(\"sem_graph\", \"node2vec_sem_graph.pkl\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = n2v.get_n2v_embedding_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = n2v.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wasp\n",
    "import json\n",
    "with open(wasp.get_data_path(\"item_pool\", \"item_pool_supp.json\"), \"r\", encoding=\"UTF-8\") as fin:\n",
    "    items = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2v_encode = partial(n2v.encode, max_length=2, drop_unk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[889, 13690]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2v_encode(\"今天/怎麼樣/由2於/呢\".split(\"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2v.model.wv.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8583]],\n",
       "\n",
       "        [[0.8965]],\n",
       "\n",
       "        [[0.9091]],\n",
       "\n",
       "        [[0.8060]],\n",
       "\n",
       "        [[0.7299]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.rand(5,1,4)\n",
    "pool = nn.MaxPool1d(4)\n",
    "pool(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qlen = []\n",
    "for item_x in items:\n",
    "    qs = sum([1 for x in item_x[\"question\"] if x in n2v.vocab])\n",
    "    olen = []\n",
    "    for opt_x in item_x[\"options\"]:\n",
    "        os = sum([1 for x in opt_x if x in n2v.vocab])\n",
    "        olen.append(os)\n",
    "    qs = qs + max(olen)\n",
    "    qlen.append(qs)\n",
    "max(qlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  889,  2605,  6428, 15600])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.tensor([stoi(x) for x in \"今天/天氣/真好/珠江\".split(\"/\")])\n",
    "emb(indices)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 889, 2605, 6428])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load(str(wasp.get_data_path(\"sem_graph\", \"node2vec_sem_graph.pkl\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab.get(\"大生\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15600, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_weights = torch.FloatTensor(model.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding.from_pretrained(emb_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 1. Got 15600 and 1 in dimension 0 at C:\\w\\1\\s\\windows\\pytorch\\aten\\src\\TH/generic/THTensor.cpp:612",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-20a273e8ca79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mn2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0munk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn2v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 15600 and 1 in dimension 0 at C:\\w\\1\\s\\windows\\pytorch\\aten\\src\\TH/generic/THTensor.cpp:612"
     ]
    }
   ],
   "source": [
    "n2v = torch.FloatTensor(model.wv.vectors)\n",
    "unk = torch.rand([1, n2v.shape[-1]])\n",
    "weights = torch.stack([n2v, unk], axis=1)\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15601, 100])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([n2v, unk], axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3375e-01, -4.2155e-01,  7.2261e-01, -9.8861e-02,  1.0304e+00,\n",
       "         -2.9366e+00, -8.6833e-01,  1.3025e+00, -4.9774e+00,  3.3973e+00,\n",
       "          2.1625e+00,  2.2947e-01,  3.1486e+00, -1.7936e+00, -1.8790e+00,\n",
       "          2.8723e+00, -2.1650e+00,  9.6802e-01,  1.3527e+00, -1.2992e+00,\n",
       "          5.0146e-01,  1.0643e-01, -3.2138e+00, -4.7904e-01, -1.8417e+00,\n",
       "         -1.2158e+00, -1.4134e+00, -2.3304e+00, -1.8468e+00, -1.0025e+00,\n",
       "         -1.8345e-03, -2.3440e+00,  9.7086e-01,  4.5544e-01,  2.8649e+00,\n",
       "         -2.4013e+00,  1.4181e+00, -1.3995e+00,  1.1271e+00,  7.2721e-01,\n",
       "          6.5078e-01,  7.9881e-01,  1.9788e+00,  2.9911e-01, -2.2252e+00,\n",
       "         -7.3543e-02,  2.2458e-01,  1.1003e-01,  4.5616e+00, -2.0423e+00,\n",
       "          1.4514e+00, -5.4374e-01, -5.4203e-01, -1.3980e+00, -1.6925e+00,\n",
       "          9.2412e-01,  2.2160e-01,  2.9389e+00, -3.7402e+00,  1.1879e-01,\n",
       "          1.3919e+00,  5.8189e-01,  1.6937e+00, -3.4922e-01,  2.3588e+00,\n",
       "          1.4474e+00,  9.2330e-01,  1.9326e-01, -1.0059e+00,  1.9396e+00,\n",
       "          2.4311e+00, -2.7124e+00, -9.8022e-01,  2.5879e+00,  2.0650e+00,\n",
       "          1.2239e-01,  1.6531e+00, -2.5073e+00,  1.1826e+00, -9.5484e-01,\n",
       "         -5.9304e-01, -2.1574e+00, -3.0355e+00, -4.6756e-01, -1.7758e+00,\n",
       "         -7.0380e-01,  7.8436e-01,  7.7222e-01, -4.3319e+00,  4.5916e-01,\n",
       "         -2.3078e+00, -2.9420e+00, -6.8946e-01, -1.2403e+00, -9.9850e-01,\n",
       "          5.3079e-01, -4.5920e-01, -9.1822e-01,  4.1443e-02,  3.2556e+00],\n",
       "        [-9.4480e-01,  3.0803e-01,  1.5803e+00, -1.0684e+00, -3.8852e+00,\n",
       "         -3.5662e-01, -1.3305e-01,  1.1934e+00, -1.1680e+00,  5.0568e+00,\n",
       "          2.4863e+00, -1.9266e+00,  2.7399e+00,  3.5626e-01,  1.1360e+00,\n",
       "         -3.7685e-01, -1.3410e+00, -5.0621e+00,  1.1350e+00, -1.8405e+00,\n",
       "          4.9509e-01,  1.6756e+00, -1.2595e-01, -3.4428e-02, -2.9542e+00,\n",
       "          2.8099e+00, -2.0306e-01,  2.0822e+00, -3.7051e+00, -2.7790e+00,\n",
       "         -4.1668e+00, -1.8323e+00, -1.3892e+00, -3.1833e+00, -1.6010e+00,\n",
       "         -8.9791e-01, -2.3028e-01, -2.4459e+00, -1.0223e+00, -1.3748e+00,\n",
       "         -4.2281e-02, -1.6193e+00,  1.6252e+00,  1.0968e+00,  3.7073e-01,\n",
       "          3.3661e+00, -3.5475e+00, -1.9973e+00, -5.0471e-01,  1.2494e+00,\n",
       "         -1.2319e+00,  1.4508e+00, -9.8942e-01,  3.0358e+00,  1.5317e+00,\n",
       "         -8.5910e-01,  3.0263e+00,  1.0288e+00,  1.6295e-01,  1.5402e+00,\n",
       "          4.2288e+00, -1.0929e+00, -2.1179e+00, -3.0358e-01, -2.1162e+00,\n",
       "          3.4314e+00, -8.6453e-01,  1.7540e+00, -3.4760e+00,  1.4869e+00,\n",
       "          1.8762e+00, -6.6530e-01,  1.6689e-01, -2.2919e+00, -7.5604e-01,\n",
       "          1.2239e+00, -5.8093e-01,  1.5921e+00,  2.0141e+00, -3.8312e+00,\n",
       "         -8.7736e-01,  7.9035e-01,  2.3213e+00, -5.9943e-01, -2.6590e+00,\n",
       "          1.6519e+00,  2.8917e+00,  2.3937e+00,  3.6596e+00,  4.7435e+00,\n",
       "          1.7455e+00,  1.5807e+00,  2.4358e+00,  1.4030e+00,  4.9826e-01,\n",
       "          8.4344e-02, -1.2586e+00, -3.1932e+00, -1.6006e+00,  4.0761e-01],\n",
       "        [ 1.3165e+00, -3.8625e+00,  5.1899e+00, -5.9056e-01, -5.8040e-01,\n",
       "          2.3687e+00,  3.7651e-01,  3.1887e+00, -7.8895e-01, -1.5132e+00,\n",
       "          1.9752e+00, -1.5924e+00,  1.7626e+00, -3.6862e-01, -1.2502e+00,\n",
       "         -3.9326e+00,  2.0946e+00,  1.3008e+00,  1.4715e+00,  3.6123e+00,\n",
       "         -1.7821e+00,  2.8756e-01,  3.1436e-01, -1.4506e+00,  9.3198e-01,\n",
       "          1.3705e+00, -6.5078e-01, -4.7316e+00,  1.4981e+00,  1.7189e+00,\n",
       "         -3.7236e+00, -1.1721e+00,  6.0558e-01,  4.9134e-01, -2.4663e+00,\n",
       "          2.3152e+00,  1.6935e+00, -3.3089e-01,  2.7309e+00, -8.5180e-01,\n",
       "          1.9132e+00,  1.6933e+00, -2.0118e+00,  3.6486e+00,  5.5181e-01,\n",
       "         -1.2956e+00,  2.2801e-01,  1.4643e+00, -2.2683e+00, -1.8186e+00,\n",
       "         -2.8698e+00,  1.0793e+00, -3.2749e+00, -4.1895e-01,  3.9071e-01,\n",
       "         -2.4918e+00, -8.0739e-01, -2.9319e+00,  1.8249e+00,  3.5650e+00,\n",
       "          6.6052e-01,  2.3959e+00, -3.3297e-01, -6.2296e-03, -1.7607e+00,\n",
       "         -2.4698e+00,  2.0355e+00, -3.6904e-01,  1.7782e-01, -3.9618e+00,\n",
       "         -1.4237e+00,  1.3524e+00, -2.1512e-02, -2.6092e+00,  1.1883e+00,\n",
       "          1.5965e+00,  2.8979e+00, -4.9529e+00, -1.3388e+00, -3.3026e+00,\n",
       "          1.2028e+00, -1.9018e+00,  1.5998e+00,  7.4595e-01, -9.4282e-01,\n",
       "          4.0060e-01, -1.5697e+00, -4.6193e-01, -9.5472e-01, -2.5504e+00,\n",
       "         -2.1810e+00,  1.4963e-01, -1.8519e-01, -1.2171e+00,  1.8859e+00,\n",
       "         -1.5990e+00, -1.0428e+00,  1.4248e-01, -6.8041e-01,  4.5576e+00]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.LongTensor([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
